<!DOCTYPE html>
<html>

  <head>
    <meta charset='utf-8'>
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <meta name="description" content="Zhounan.GitHub.com : resume">

    <link rel="stylesheet" type="text/css" media="screen" href="stylesheets/stylesheet.css">

    <title>Zhounan.GitHub.com</title>
  </head>

  <body>

    <!-- HEADER -->
    <div id="header_wrap" class="outer">
        <header class="inner">
          <a id="forkme_banner" href="https://github.com/supernan/zhounan.github.com">View on GitHub</a>

          <h1 id="project_title">Nan Zhou</h1>
         <!--  <h2 id="project_tagline">resume</h2> -->

           <!--  <section id="downloads">
              <a class="zip_download_link" href="https://github.com/supernan/zhounan.github.com/zipball/master">Download this project as a .zip file</a>
              <a class="tar_download_link" href="https://github.com/supernan/zhounan.github.com/tarball/master">Download this project as a tar.gz file</a>
            </section> -->
        </header>
    </div>

    <!-- MAIN CONTENT -->
    <div id="main_content_wrap" class="outer">
      <section id="main_content" class="inner">
        <h3>
<a id="welcome-to-github-pages" class="anchor" href="#welcome-to-github-pages" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>OBJECTIVE</h3>

<p>A position in the field of computers with special interests in natural language pro- cessing, information retrieval, and data mining.</p>

<h3>
<a id="designer-templates" class="anchor" href="#designer-templates" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>EDUCATION</h3>

<p>During the Master’s Degree
Institute of Computing Technology, Chinese Academy of Sciences, 2014-2017 Concentration: Natural Language Processing, Information Retrieval</p>


<p>Bachelor of Computer Science
South China University of Technology, 2010-2014 Concentration: Information Security</p>

<h3>
<a id="creating-pages-manually" class="anchor" href="#creating-pages-manually" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>COMPUTER SKILLS</h3>

<p>Languages & Software: C++, Java, Python, Database, Data Structure, Fundamental Algorithms, Basic Knowledge of Machine Learning and Data Mining
Operating Systems: Windows, Linux.</p>

<h3>
<a id="authors-and-contributors" class="anchor" href="#authors-and-contributors" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>INTERNSHIP EXPERIENCE</h3>

<h6>Development Intern 2015.1-2015.6 Baidu AD risk control team, strategy group</h6>
<p>• Web information and material text information of risk control.</p>
<p>• Risk control model development and tuning.Identify the text information of gambling and digital fraud.</p>
<p>• Analyze the ads Landing Page, summarized the characteristics of the illegal Page</p>
<h6>C++ Development Intern 2013.9-2013.11 Kingsoft office software (WPS)</h6>
<p>• Code maintenance of WPS revised text function.</p>
<p>• Unit test of column width calculation algorithms.</p>
<p>• Familiar with the logic of document revision and form layout, fix bugs.</p>

<h3>
<a id="support-or-contact" class="anchor" href="#support-or-contact" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>PROJECT EXPERIENCE</h3>
<h6>Alibaba big data competition of Sina weibo interaction prediction   2015.9-2015.11</h6>
<p>• Given the social relation data of all users,blogs of 4 months and the correspond- ing post time. Our job is training a model to predict users’ interactive numbers one month later. Amount of data is 20 million. We are No.4.</p>
<p>• Based on Random Forests and GBDT to build models.Using feedback and lin- ear weighted methods to achieve ensemble learning. Alibaba Tianchi ODPS platform can help us complete development task, data analysis and feature extraction with SQL ,Mapreduce and algorithm components.</p>
<h6>Using topic model PLSA Model With Background Language Model for event topic discovery   2015.9-2015.10
</h6>
<p>• Find the subtopic of documents which describe a same event.Different subtopics tell us various information.Then we need to generate the topic TAG of each subtopic.Finally, we may cluster the corresponding documents.</p>
<p>• Using PLSA Model With Background to find key words. Build a matrix to describe co-occurrence of every key word in a sentence level. Then cluster words by union and finding set.</p>
<p>• Spark to Parallelization.</p>
<h6>News corpus based event classification and discovery 2015.11-2015.12</h6>
<p>• First of all, classify all documents(a total of 10 categories, 67 small classes). Cluster the documents which describe the same event of each class. At the end, based on seeds documents we have found, expand event corpus.</p>
<p>• Achieve text classification based on SVM.Use key words list to filer spam. Ex- tracting event triggers with CRF++, based on those triggers, we can filer the documents again. After that, entities(name, place name, organization name, etc.) can be used to build a event classification tree. Each leaf node is corpus of one single event(seeds docs).To expand seeds, we map each doc to feature space. Based on cosin distance, we may find more relative documents.</p>
<h6>SIGHAN Task, person attribute extraction 2014.8-2014.9</h6>
<p>• From a given text extract the relevant attributes of a specific person by patterns
or models.For example: name, date of birth, title, birth city, etc..</p>
<p>• First, use rules and some patterns to extract religious property of someone. For other attributes, train CRF model with labeled data. and then based on machine learning and statistical methods to automatically obtain the related fields. all scripts have been completed by Python.</p>
<h6>Rule filter module and topic discovery module of opinion detection system 2015.9</h6>
<p>• Rule filter module is based on double array Trie and basic rules. The component is used to filter raw texts. Users can write some patterns to filter messages from message queue or database.</p>
<p>• The key of topic discovery module is clustering documents. First we use Cannopy algorithm to cut documents to some parts. For each of them, K- means is our choice to find topic. For preprocess, Simhash and SVD can be used to improve performance.</p>

      </section>
    </div>

    <!-- FOOTER  -->
    <div id="footer_wrap" class="outer">
      <footer class="inner">
        <p class="copyright">Zhounan.GitHub.com maintained by <a href="https://github.com/supernan">supernan</a></p>
        <p>Published with <a href="https://pages.github.com">GitHub Pages</a></p>
      </footer>
    </div>

    

  </body>
</html>
